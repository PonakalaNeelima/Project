# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/161sRu363KiaguUfeOLOYzF1NLY6gq0hq

# Importing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import StratifiedKFold, cross_val_predict

# Load dataset
df1 = pd.read_csv("https://raw.githubusercontent.com/PonakalaNeelima/Project/refs/heads/master/waterQuality1.csv")

df2 = pd.read_csv("https://raw.githubusercontent.com/PonakalaNeelima/Project/refs/heads/master/water_potability.csv")

"""# Data Exploration"""

df1.info()

df2.info()

"""# Data Preprocessing"""

#converting object to numeric
df1['ammonia'] = pd.to_numeric(df1['ammonia'], errors='coerce')
df1['is_safe'] = pd.to_numeric(df1['is_safe'], errors='coerce')

#Handling NaN values
df1.fillna(df1.median(), inplace=True)

df1.info()

#Handling NaN values
df2.fillna(df2.median(), inplace=True)

df2.info()

df1 = df1.drop(columns=['aluminium', 'ammonia', 'barium', 'cadmium', 'chromium', 'copper', 'flouride', 'viruses', 'nitrites', 'perchlorate', 'radium', 'selenium', 'silver', 'uranium'])

# Insert the columns from df2 at the beginning of df1
df1.insert(0, 'ph', df2['ph'])

df1.insert(1, 'hardness', df2['Hardness'])

df1.insert(2, 'turbidity', df2['Turbidity'])

df=df1

df.info()

#Handling NaN values
df.fillna(df.median(), inplace=True)

df.info()

df.to_csv("output.csv", index=False)

"""# Data visualization

"""

is_safe_counts = df['is_safe'].value_counts()

# Create the bar graph
plt.figure(figsize=(8, 6))
sns.barplot(x=is_safe_counts.index, y=is_safe_counts.values, palette='viridis')
plt.title('Count of Safe and Unsafe Water Samples')
plt.xlabel('Is Safe')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])
plt.show()

"""# Feature Scaling & Splitting"""

# Split into features and target
X = df.drop('is_safe', axis=1)
y = df['is_safe']

# Standard scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Show the shapes of the resulting arrays
print("Training features shape:", X_train.shape)
print("Testing features shape:", X_test.shape)
print("Training target shape:", y_train.shape)
print("Testing target shape:", y_test.shape)

#checking if splits are correct
print(f"Train size: {round(len(X_train) / len(X) * 100)}% \n\
Test size: {round(len(X_test) / len(X) * 100)}%")

"""# Building Model"""

# Instnatiating the models
logistic_regression = LogisticRegression()
svm = SVC(probability=True)
tree = DecisionTreeClassifier()
forest=RandomForestClassifier()

# Training the models
logistic_regression.fit(X_train, y_train)
svm.fit(X_train, y_train)
tree.fit(X_train, y_train)
forest.fit(X_train, y_train)

# Making predictions with each model
log_reg_preds = logistic_regression.predict(X_test)
svm_preds = svm.predict(X_test)
tree_preds = tree.predict(X_test)
forest_preds = forest.predict(X_test)

"""# Model Evaluation"""

model_preds = {
    "Logistic Regression": log_reg_preds,
    "Support Vector Machine": svm_preds,
    "Decision Tree": tree_preds,
    "Random Forest": forest_preds
}

for model, preds in model_preds.items():
    print(f"{model} :\nClassification_report\n{classification_report(y_test, preds)}", sep="\n")
    print(f"The accuracy of the model is {accuracy_score(y_test,preds)*100}% ",sep="\n")

"""# Ensemble Learning

"""

# Create a list of base models
base_models = [logistic_regression, svm, tree, forest]

# Cross-validation predictions for each base model
def create_stack_dataset(base_models, X, y, n_splits=5):
    stack_X = []
    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    for model in base_models:
        # Get out-of-fold predictions for each base model
        oof_predictions = cross_val_predict(model, X, y, cv=kf, method='predict_proba')
        stack_X.append(oof_predictions[:, 1])  # Use the probability of class 1 (safe water)

    # Stack the predictions from all models
    stack_X = np.column_stack(stack_X)
    return stack_X
# Generate new dataset for stacking (using predictions from cross-validation)
X_stack = create_stack_dataset(base_models, X_train, y_train, n_splits=5)

# Train a meta-model on the new stacked dataset
meta_model = LogisticRegression()
meta_model.fit(X_stack, y_train)

# Get the predictions for the test set by using the trained base models
base_model_preds_test = []
for model in base_models:
    model.fit(X_train, y_train)  # Fit the model on the full training data
    base_model_preds_test.append(model.predict_proba(X_test)[:, 1])  # Use probability of class 1

# Create a new dataset for the meta-model's prediction
X_test_stack = np.column_stack(base_model_preds_test)

# Get the final predictions from the meta-model
y_stack_pred = meta_model.predict(X_test_stack)

# Evaluate the meta-model
print(f"Accuracy of Stacked Model: {accuracy_score(y_test, y_stack_pred)*100:.2f}%")
print("\nClassification Report for Stacked Model:")
print(classification_report(y_test, y_stack_pred))

"""# Deployment"""

import joblib
filename1 = 'Stacked_Model.joblib'
joblib.dump(meta_model,filename1)
filename2 = 'lr.joblib'
joblib.dump(logistic_regression,filename2)
filename3 = 'svm.joblib'
joblib.dump(svm,filename3)
filename4 = 'tree.joblib'
joblib.dump(tree,filename4)
filename5 = 'forest.joblib'
joblib.dump(forest,filename5)

